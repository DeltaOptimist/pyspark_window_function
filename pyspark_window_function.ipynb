{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbjM-CEzuC4M",
        "outputId": "6ab1e9db-9d45-411e-dcb8-91a2755c375f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 38 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 55.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=8eb9dd6af8666e481941bfdfebb797c25e63a5e43febdb7f81e8549a0b2bf9d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "Ryf3p27Cuk7W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('SparkWindowFunction').getOrCreate()"
      ],
      "metadata": {
        "id": "QwjG2UFluwVp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
        "    (\"Madhav\", \"Sales\", 4600),  \\\n",
        "    (\"Rohit\", \"Sales\", 4100),   \\\n",
        "    (\"Mitesh\", \"Finance\", 3000),  \\\n",
        "    (\"Gursheen\", \"Sales\", 3000),    \\\n",
        "    (\"Yash\", \"Finance\", 3300),  \\\n",
        "    (\"Ben\", \"Finance\", 3900),    \\\n",
        "    (\"Pnadey\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000),\\\n",
        "    (\"Shah\", \"Sales\", 4100) \\\n",
        "  )"
      ],
      "metadata": {
        "id": "KKMfOqAuu3v0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns= [\"employee_name\", \"department\", \"salary\"]"
      ],
      "metadata": {
        "id": "c4_iwK9dzmLh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data = simpleData, schema = columns)"
      ],
      "metadata": {
        "id": "oGLGJN4rzz1h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55r-tk3Kz5k7",
        "outputId": "30ff7edd-0356-4e0c-af2f-41820b7d9911"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Madhav       |Sales     |4600  |\n",
            "|Rohit        |Sales     |4100  |\n",
            "|Mitesh       |Finance   |3000  |\n",
            "|Gursheen     |Sales     |3000  |\n",
            "|Yash         |Finance   |3300  |\n",
            "|Ben          |Finance   |3900  |\n",
            "|Pnadey       |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Shah         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number"
      ],
      "metadata": {
        "id": "SHqOVsfEz9Vu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")"
      ],
      "metadata": {
        "id": "wQe_FTD_0I3G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"row_number\",row_number().over(windowSpec)).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MzW93fl0MRb",
        "outputId": "dca80998-885d-4222-93d8-50f9710917d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|row_number|\n",
            "+-------------+----------+------+----------+\n",
            "|Mitesh       |Finance   |3000  |1         |\n",
            "|Yash         |Finance   |3300  |2         |\n",
            "|Ben          |Finance   |3900  |3         |\n",
            "|Kumar        |Marketing |2000  |1         |\n",
            "|Pnadey       |Marketing |3000  |2         |\n",
            "|James        |Sales     |3000  |1         |\n",
            "|Gursheen     |Sales     |3000  |2         |\n",
            "|Rohit        |Sales     |4100  |3         |\n",
            "|Shah         |Sales     |4100  |4         |\n",
            "|Madhav       |Sales     |4600  |5         |\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "rank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties."
      ],
      "metadata": {
        "id": "56kGhg1V20D1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rank\n",
        "df.withColumn(\"rank\",rank().over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7UxMfPx0Upn",
        "outputId": "72caf247-93cf-49d8-d2dc-cdf0deaa0263"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|rank|\n",
            "+-------------+----------+------+----+\n",
            "|       Mitesh|   Finance|  3000|   1|\n",
            "|         Yash|   Finance|  3300|   2|\n",
            "|          Ben|   Finance|  3900|   3|\n",
            "|        Kumar| Marketing|  2000|   1|\n",
            "|       Pnadey| Marketing|  3000|   2|\n",
            "|        James|     Sales|  3000|   1|\n",
            "|     Gursheen|     Sales|  3000|   1|\n",
            "|        Rohit|     Sales|  4100|   3|\n",
            "|         Shah|     Sales|  4100|   3|\n",
            "|       Madhav|     Sales|  4600|   5|\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dense_rank() window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to rank() function difference being rank function leaves gaps in rank when there are ties."
      ],
      "metadata": {
        "id": "aRPKYHcM2nGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import dense_rank\n",
        "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWRHslVw0aaR",
        "outputId": "16005013-183c-4496-f056-25ad4e592ba2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|dense_rank|\n",
            "+-------------+----------+------+----------+\n",
            "|       Mitesh|   Finance|  3000|         1|\n",
            "|         Yash|   Finance|  3300|         2|\n",
            "|          Ben|   Finance|  3900|         3|\n",
            "|        Kumar| Marketing|  2000|         1|\n",
            "|       Pnadey| Marketing|  3000|         2|\n",
            "|        James|     Sales|  3000|         1|\n",
            "|     Gursheen|     Sales|  3000|         1|\n",
            "|        Rohit|     Sales|  4100|         2|\n",
            "|         Shah|     Sales|  4100|         2|\n",
            "|       Madhav|     Sales|  4600|         3|\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import percent_rank\n",
        "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt6EMdEp0xzR",
        "outputId": "ddc5d10a-fc92-4e13-be30-a3c715a9e619"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+------------+\n",
            "|employee_name|department|salary|percent_rank|\n",
            "+-------------+----------+------+------------+\n",
            "|       Mitesh|   Finance|  3000|         0.0|\n",
            "|         Yash|   Finance|  3300|         0.5|\n",
            "|          Ben|   Finance|  3900|         1.0|\n",
            "|        Kumar| Marketing|  2000|         0.0|\n",
            "|       Pnadey| Marketing|  3000|         1.0|\n",
            "|        James|     Sales|  3000|         0.0|\n",
            "|     Gursheen|     Sales|  3000|         0.0|\n",
            "|        Rohit|     Sales|  4100|         0.5|\n",
            "|         Shah|     Sales|  4100|         0.5|\n",
            "|       Madhav|     Sales|  4600|         1.0|\n",
            "+-------------+----------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ntile() window function returns the relative rank of result rows within a window partition. In below example we have used 2 as an argument to ntile hence it returns ranking between 2 values (1 and 2)"
      ],
      "metadata": {
        "id": "30Bga0Tv2glz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import ntile\n",
        "df.withColumn(\"ntile\",ntile(2).over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h5LwgMt04r_",
        "outputId": "b4ad3179-7788-4fd4-98d9-e87816bf9f1e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+-----+\n",
            "|employee_name|department|salary|ntile|\n",
            "+-------------+----------+------+-----+\n",
            "|       Mitesh|   Finance|  3000|    1|\n",
            "|         Yash|   Finance|  3300|    1|\n",
            "|          Ben|   Finance|  3900|    2|\n",
            "|        Kumar| Marketing|  2000|    1|\n",
            "|       Pnadey| Marketing|  3000|    2|\n",
            "|        James|     Sales|  3000|    1|\n",
            "|     Gursheen|     Sales|  3000|    1|\n",
            "|        Rohit|     Sales|  4100|    1|\n",
            "|         Shah|     Sales|  4100|    2|\n",
            "|       Madhav|     Sales|  4600|    2|\n",
            "+-------------+----------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark Window Analytic functions\n",
        "\n",
        "cume_dist() window function is used to get the cumulative distribution of values within a window partition."
      ],
      "metadata": {
        "id": "qjj6MqKD2-_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import cume_dist    \n",
        "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJQRBytt09vh",
        "outputId": "4a88d799-b2d6-4ceb-d49a-8a4b8cc644da"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+------------------+\n",
            "|employee_name|department|salary|         cume_dist|\n",
            "+-------------+----------+------+------------------+\n",
            "|       Mitesh|   Finance|  3000|0.3333333333333333|\n",
            "|         Yash|   Finance|  3300|0.6666666666666666|\n",
            "|          Ben|   Finance|  3900|               1.0|\n",
            "|        Kumar| Marketing|  2000|               0.5|\n",
            "|       Pnadey| Marketing|  3000|               1.0|\n",
            "|        James|     Sales|  3000|               0.4|\n",
            "|     Gursheen|     Sales|  3000|               0.4|\n",
            "|        Rohit|     Sales|  4100|               0.8|\n",
            "|         Shah|     Sales|  4100|               0.8|\n",
            "|       Madhav|     Sales|  4600|               1.0|\n",
            "+-------------+----------+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windowSpecAgg  = Window.partitionBy(\"department\")\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
        "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
        "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7qKsK9q1Z5q",
        "outputId": "ec832a36-fbe0-457c-c3d6-f1d9d5f48bc8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+----+----+\n",
            "|department|   avg|  sum| min| max|\n",
            "+----------+------+-----+----+----+\n",
            "|   Finance|3400.0|10200|3000|3900|\n",
            "| Marketing|2500.0| 5000|2000|3000|\n",
            "|     Sales|3760.0|18800|3000|4600|\n",
            "+----------+------+-----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OB0jIXWS1lHT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}